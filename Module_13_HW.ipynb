{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Module 13 HW.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WxC1b2S_cNos"
      },
      "source": [
        "1. The term accuracy means to be very precise like throwing a dart dead center on a dart board.\n",
        "2. The term precision means to be right on the target like throwing a basketball into the hoop.\n",
        "3. The term recall means to make an order to return to a place like bringing back an item from the perosn to the store because of a reason.\n",
        "4. An confusion matrix is a specific table layout that allows visualization of the performance of an algorithm.\n",
        "5. An example of an type one error would be an math teacher saying your answer for a math problem is wrong, but is actually right.\n",
        "6. An example of an type two error would be an teacher saying your answer for a problem is correct, but is actually wrong.\n",
        "7. The bias vs. variance tradeoff is the variance of how much the predictions for a given point vary between different realizations of each model.\n",
        "8. The reason why we use an train.test,split() function when analyzing data is because Sklearn models will divide the dataset automatically instead of doing it manually.\n",
        "9. The point of splitting data is to improve its performance and reduce the chance of database file corruption."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_7o50RPLjND8"
      },
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "y_pred = [0, 2, 1, 3]\n",
        "y_true = [0, 1, 2, 3]\n",
        "accuracy_score(y_true, y_pred)\n",
        "0.5\n",
        "accuracy_score(y_true, y_pred, normalize=False)\n",
        "2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gCJljxj9mCzu"
      },
      "source": [
        "def classification_scores(gts, preds, labels):\n",
        "    accuracy        = metrics.accuracy_score(gts,  preds)\n",
        "    class_accuracies = []\n",
        "    for lab in labels: # TODO Fix\n",
        "        class_accuracies.append(metrics.accuracy_score(gts[gts == lab], preds[gts == lab]))\n",
        "    class_accuracies = np.array(class_accuracies)\n",
        "\n",
        "    f1_micro        = metrics.f1_score(gts,        preds, average='micro')\n",
        "    precision_micro = metrics.precision_score(gts, preds, average='micro')\n",
        "    recall_micro    = metrics.recall_score(gts,    preds, average='micro')\n",
        "    f1_macro        = metrics.f1_score(gts,        preds, average='macro')\n",
        "    precision_macro = metrics.precision_score(gts, preds, average='macro')\n",
        "    recall_macro    = metrics.recall_score(gts,    preds, average='macro')\n",
        "\n",
        "    # class wise score\n",
        "    f1s        = metrics.f1_score(gts,        preds, average=None)\n",
        "    precisions = metrics.precision_score(gts, preds, average=None)\n",
        "    recalls    = metrics.recall_score(gts,    preds, average=None)\n",
        "\n",
        "    confusion = metrics.confusion_matrix(gts,preds, labels=labels)\n",
        "\n",
        "    #TODO confusion matrix, recall, precision\n",
        "    return accuracy, f1_micro, precision_micro, recall_micro, f1_macro, precision_macro, recall_macro, confusion, class_accuracies, f1s, precisions, recalls "
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dvJi-EMxm5Xs"
      },
      "source": [
        "def eval_class(ids_to_eval, model, z_obs):\n",
        "    \"\"\"\n",
        "    Evaluate the model's classification performance.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    ids_to_eval: np.array\n",
        "        The indices of the nodes whose predictions will be evaluated.\n",
        "\n",
        "    model: GCN\n",
        "        The model to evaluate.\n",
        "\n",
        "    z_obs: np.array\n",
        "        The labels of the nodes in ids_to_eval\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    [f1_micro, f1_macro] scores\n",
        "\n",
        "    \"\"\"\n",
        "    test_pred = model.predictions.eval(session=model.session, feed_dict={model.node_ids: ids_to_eval}).argmax(1)\n",
        "    test_real = z_obs[ids_to_eval]\n",
        "\n",
        "    return f1_score(test_real, test_pred, average='micro'), f1_score(test_real, test_pred, average='macro') "
      ],
      "execution_count": 9,
      "outputs": []
    }
  ]
}